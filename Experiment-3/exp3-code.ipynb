{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJG7jOk7DJqY",
        "outputId": "2c9bdb59-f7e7-4f12-b465-80353e441ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./cifar10.tgz\n",
            "(tensor([[[0.4941, 0.4784, 0.4941,  ..., 0.4863, 0.4706, 0.4588],\n",
            "         [0.4784, 0.4667, 0.4745,  ..., 0.4863, 0.4784, 0.4588],\n",
            "         [0.4784, 0.4745, 0.4745,  ..., 0.4941, 0.4824, 0.4745],\n",
            "         ...,\n",
            "         [0.4941, 0.4941, 0.5020,  ..., 0.5216, 0.4784, 0.4471],\n",
            "         [0.4902, 0.4941, 0.4980,  ..., 0.5020, 0.4745, 0.4471],\n",
            "         [0.4824, 0.4824, 0.4941,  ..., 0.5020, 0.4941, 0.4745]],\n",
            "\n",
            "        [[0.4627, 0.4510, 0.4667,  ..., 0.4627, 0.4471, 0.4353],\n",
            "         [0.4510, 0.4392, 0.4471,  ..., 0.4627, 0.4549, 0.4353],\n",
            "         [0.4510, 0.4471, 0.4471,  ..., 0.4706, 0.4588, 0.4510],\n",
            "         ...,\n",
            "         [0.4627, 0.4627, 0.4706,  ..., 0.4902, 0.4471, 0.4157],\n",
            "         [0.4588, 0.4627, 0.4667,  ..., 0.4706, 0.4431, 0.4157],\n",
            "         [0.4510, 0.4510, 0.4627,  ..., 0.4667, 0.4627, 0.4431]],\n",
            "\n",
            "        [[0.4314, 0.4235, 0.4353,  ..., 0.4157, 0.4000, 0.3882],\n",
            "         [0.4196, 0.4078, 0.4157,  ..., 0.4157, 0.4078, 0.3882],\n",
            "         [0.4196, 0.4157, 0.4157,  ..., 0.4235, 0.4118, 0.4039],\n",
            "         ...,\n",
            "         [0.4196, 0.4196, 0.4275,  ..., 0.4471, 0.4039, 0.3725],\n",
            "         [0.4157, 0.4196, 0.4235,  ..., 0.4275, 0.4000, 0.3725],\n",
            "         [0.4078, 0.4078, 0.4196,  ..., 0.4275, 0.4196, 0.4000]]]), 0)\n",
            "45000\n",
            "\n",
            "\n",
            "5000\n",
            "Using device: cuda\n",
            "Epoch 1/400, Loss: 2.302716664969921\n",
            "Epoch 2/400, Loss: 2.3027043850584463\n",
            "Epoch 3/400, Loss: 2.302713464606892\n",
            "Epoch 4/400, Loss: 2.302711242979223\n",
            "Epoch 5/400, Loss: 2.3027064495465974\n",
            "Epoch 6/400, Loss: 2.302707442505793\n",
            "Epoch 7/400, Loss: 2.302713992243463\n",
            "Epoch 8/400, Loss: 2.3027129952203143\n",
            "Epoch 9/400, Loss: 2.3027156550775874\n",
            "Epoch 10/400, Loss: 2.302712032063441\n",
            "Epoch 11/400, Loss: 2.3027136393568735\n",
            "Epoch 12/400, Loss: 2.3027117401361465\n",
            "Epoch 13/400, Loss: 2.302707474340092\n",
            "Epoch 14/400, Loss: 2.302716329016469\n",
            "Epoch 15/400, Loss: 2.3027098470113496\n",
            "Epoch 16/400, Loss: 2.302710185674104\n",
            "Epoch 17/400, Loss: 2.3027123788541015\n",
            "Epoch 18/400, Loss: 2.3027063621716066\n",
            "Epoch 19/400, Loss: 2.302719622850418\n",
            "Epoch 20/400, Loss: 2.3027219542048196\n",
            "Epoch 21/400, Loss: 2.3027108094908972\n",
            "Epoch 22/400, Loss: 2.302718684077263\n",
            "Epoch 23/400, Loss: 2.3027144352143463\n",
            "Epoch 24/400, Loss: 2.3027108311653137\n",
            "Epoch 25/400, Loss: 2.3027124669064176\n",
            "Epoch 26/400, Loss: 2.302714903923598\n",
            "Epoch 27/400, Loss: 2.3027090701189907\n",
            "Epoch 28/400, Loss: 2.3027090301567856\n",
            "Epoch 29/400, Loss: 2.3027061203664\n",
            "Epoch 30/400, Loss: 2.302704162218354\n",
            "Epoch 31/400, Loss: 2.3027119812640278\n",
            "Epoch 32/400, Loss: 2.3027176091616806\n",
            "Epoch 33/400, Loss: 2.302707450633699\n",
            "Epoch 34/400, Loss: 2.3027101877060803\n",
            "Epoch 35/400, Loss: 2.302713245153427\n",
            "Epoch 36/400, Loss: 2.30271447991783\n",
            "Epoch 37/400, Loss: 2.302713642743501\n",
            "Epoch 38/400, Loss: 2.302705731581558\n",
            "Epoch 39/400, Loss: 2.3027095286683603\n",
            "Epoch 40/400, Loss: 2.3027083799242973\n",
            "Epoch 41/400, Loss: 2.30271631072868\n",
            "Epoch 42/400, Loss: 2.3027126545255836\n",
            "Epoch 43/400, Loss: 2.3027056936513293\n",
            "Epoch 44/400, Loss: 2.302715319124135\n",
            "Epoch 45/400, Loss: 2.3027153848247095\n",
            "Epoch 46/400, Loss: 2.302710411223498\n",
            "Epoch 47/400, Loss: 2.302714940499176\n",
            "Epoch 48/400, Loss: 2.3027067231861027\n",
            "Epoch 49/400, Loss: 2.3027074817906725\n",
            "Epoch 50/400, Loss: 2.3027132112871516\n",
            "Epoch 51/400, Loss: 2.302711979232051\n",
            "Epoch 52/400, Loss: 2.3027144027027218\n",
            "Epoch 53/400, Loss: 2.3027144074440002\n",
            "Epoch 54/400, Loss: 2.302711808546023\n",
            "Epoch 55/400, Loss: 2.3027079471132974\n",
            "Epoch 56/400, Loss: 2.3027153035456482\n",
            "Epoch 57/400, Loss: 2.3027086325667123\n",
            "Epoch 58/400, Loss: 2.3027160411531273\n",
            "Epoch 59/400, Loss: 2.3027064590291544\n",
            "Epoch 60/400, Loss: 2.302712645043026\n",
            "Epoch 61/400, Loss: 2.302716674452478\n",
            "Epoch 62/400, Loss: 2.302710636772893\n",
            "Epoch 63/400, Loss: 2.3027176518331873\n",
            "Epoch 64/400, Loss: 2.302720621228218\n",
            "Epoch 65/400, Loss: 2.3027035092765633\n",
            "Epoch 66/400, Loss: 2.302712871269746\n",
            "Epoch 67/400, Loss: 2.302702848206867\n",
            "Epoch 68/400, Loss: 2.3027100055055185\n",
            "Epoch 69/400, Loss: 2.3027088703079657\n",
            "Epoch 70/400, Loss: 2.3027072718197648\n",
            "Epoch 71/400, Loss: 2.3027078489010986\n",
            "Epoch 72/400, Loss: 2.302712721580809\n",
            "Epoch 73/400, Loss: 2.3027127093889495\n",
            "Epoch 74/400, Loss: 2.3027121594006363\n",
            "Epoch 75/400, Loss: 2.302712607790123\n",
            "Epoch 76/400, Loss: 2.30270974134857\n",
            "Epoch 77/400, Loss: 2.302710258147933\n",
            "Epoch 78/400, Loss: 2.302711784162305\n",
            "Epoch 79/400, Loss: 2.302711337127469\n",
            "Epoch 80/400, Loss: 2.3027114312757146\n",
            "Epoch 81/400, Loss: 2.302710625258359\n",
            "Epoch 82/400, Loss: 2.3027079193429514\n",
            "Epoch 83/400, Loss: 2.302706670354713\n",
            "Epoch 84/400, Loss: 2.302715726874091\n",
            "Epoch 85/400, Loss: 2.3027068586512045\n",
            "Epoch 86/400, Loss: 2.3027102161537516\n",
            "Epoch 87/400, Loss: 2.3027179410511796\n",
            "Epoch 88/400, Loss: 2.3027144501155075\n",
            "Epoch 89/400, Loss: 2.302709410136396\n",
            "Epoch 90/400, Loss: 2.3027087917382065\n",
            "Epoch 91/400, Loss: 2.3027099404822695\n",
            "Epoch 92/400, Loss: 2.302711561322212\n",
            "Epoch 93/400, Loss: 2.3027027540586213\n",
            "Epoch 94/400, Loss: 2.3027101768688722\n",
            "Epoch 95/400, Loss: 2.302708224816756\n",
            "Epoch 96/400, Loss: 2.302713231606917\n",
            "Epoch 97/400, Loss: 2.3027198267253963\n",
            "Epoch 98/400, Loss: 2.302710281854326\n",
            "Epoch 99/400, Loss: 2.302708509970795\n",
            "Epoch 100/400, Loss: 2.3027081801132723\n",
            "Epoch 101/400, Loss: 2.302714975720102\n",
            "Epoch 102/400, Loss: 2.302711060778661\n",
            "Epoch 103/400, Loss: 2.3027064244855535\n",
            "Epoch 104/400, Loss: 2.302712758156386\n",
            "Epoch 105/400, Loss: 2.302710818296129\n",
            "Epoch 106/400, Loss: 2.3027105446566236\n",
            "Epoch 107/400, Loss: 2.3027153394439\n",
            "Epoch 108/400, Loss: 2.302707329392433\n",
            "Epoch 109/400, Loss: 2.302714592353864\n",
            "Epoch 110/400, Loss: 2.302711616862904\n",
            "Epoch 111/400, Loss: 2.3027170964262704\n",
            "Epoch 112/400, Loss: 2.3027040660381317\n",
            "Epoch 113/400, Loss: 2.3027147142724558\n",
            "Epoch 114/400, Loss: 2.3027084557847544\n",
            "Epoch 115/400, Loss: 2.3027128997174175\n",
            "Epoch 116/400, Loss: 2.302710075270046\n",
            "Epoch 117/400, Loss: 2.3027090829881756\n",
            "Epoch 118/400, Loss: 2.3027118573134597\n",
            "Epoch 119/400, Loss: 2.302714239467274\n",
            "Epoch 120/400, Loss: 2.3027099160985514\n",
            "Epoch 121/400, Loss: 2.302718774838881\n",
            "Epoch 122/400, Loss: 2.302711123092608\n",
            "Epoch 123/400, Loss: 2.3027125149965286\n",
            "Epoch 124/400, Loss: 2.3027138940312644\n",
            "Epoch 125/400, Loss: 2.302710171450268\n",
            "Epoch 126/400, Loss: 2.302709822627631\n",
            "Epoch 127/400, Loss: 2.302713296630166\n",
            "Epoch 128/400, Loss: 2.3027116947553377\n",
            "Epoch 129/400, Loss: 2.30271053314209\n",
            "Epoch 130/400, Loss: 2.3027087179097263\n",
            "Epoch 131/400, Loss: 2.302706339819865\n",
            "Epoch 132/400, Loss: 2.3027171167460354\n",
            "Epoch 133/400, Loss: 2.30271032046188\n",
            "Epoch 134/400, Loss: 2.302715138278224\n",
            "Epoch 135/400, Loss: 2.30271381478418\n",
            "Epoch 136/400, Loss: 2.302713107656349\n",
            "Epoch 137/400, Loss: 2.3027092435143213\n",
            "Epoch 138/400, Loss: 2.302711229432713\n",
            "Epoch 139/400, Loss: 2.302719183266163\n",
            "Epoch 140/400, Loss: 2.3027193038301035\n",
            "Epoch 141/400, Loss: 2.3027125271883877\n",
            "Epoch 142/400, Loss: 2.302710548043251\n",
            "Epoch 143/400, Loss: 2.3027102283456107\n",
            "Epoch 144/400, Loss: 2.3027063154361467\n",
            "Epoch 145/400, Loss: 2.3027126145633785\n",
            "Epoch 146/400, Loss: 2.3027031936428766\n",
            "Epoch 147/400, Loss: 2.30270781097087\n",
            "Epoch 148/400, Loss: 2.3027092672207137\n",
            "Epoch 149/400, Loss: 2.3027182736180047\n",
            "Epoch 150/400, Loss: 2.3027118864384564\n",
            "Epoch 151/400, Loss: 2.302708600055088\n",
            "Epoch 152/400, Loss: 2.3027140538800848\n",
            "Epoch 153/400, Loss: 2.3027072569186036\n",
            "Epoch 154/400, Loss: 2.302720256827094\n",
            "Epoch 155/400, Loss: 2.302706641229716\n",
            "Epoch 156/400, Loss: 2.3027148524468597\n",
            "Epoch 157/400, Loss: 2.3027131753888996\n",
            "Epoch 158/400, Loss: 2.302711868827993\n",
            "Epoch 159/400, Loss: 2.302706121721051\n",
            "Epoch 160/400, Loss: 2.3027077601714567\n",
            "Epoch 161/400, Loss: 2.302713679996404\n",
            "Epoch 162/400, Loss: 2.3027110160751776\n",
            "Epoch 163/400, Loss: 2.302713054824959\n",
            "Epoch 164/400, Loss: 2.3027071370319887\n",
            "Epoch 165/400, Loss: 2.3027079572731797\n",
            "Epoch 166/400, Loss: 2.302716228094968\n",
            "Epoch 167/400, Loss: 2.302712606435472\n",
            "Epoch 168/400, Loss: 2.302718184888363\n",
            "Epoch 169/400, Loss: 2.302714862606742\n",
            "Epoch 170/400, Loss: 2.3027128753336994\n",
            "Epoch 171/400, Loss: 2.3027065592733296\n",
            "Epoch 172/400, Loss: 2.3027099039066923\n",
            "Epoch 173/400, Loss: 2.3027049296281557\n",
            "Epoch 174/400, Loss: 2.302713856101036\n",
            "Epoch 175/400, Loss: 2.302704701369459\n",
            "Epoch 176/400, Loss: 2.302712970836596\n",
            "Epoch 177/400, Loss: 2.302713206545873\n",
            "Epoch 178/400, Loss: 2.302707886831327\n",
            "Epoch 179/400, Loss: 2.30271135202863\n",
            "Epoch 180/400, Loss: 2.3027054396542637\n",
            "Epoch 181/400, Loss: 2.3027056977152824\n",
            "Epoch 182/400, Loss: 2.3027173328128727\n",
            "Epoch 183/400, Loss: 2.302719886330041\n",
            "Epoch 184/400, Loss: 2.3027214909141716\n",
            "Epoch 185/400, Loss: 2.3027134944092142\n",
            "Epoch 186/400, Loss: 2.302712870592421\n",
            "Epoch 187/400, Loss: 2.302711410278624\n",
            "Epoch 188/400, Loss: 2.3027088452469218\n",
            "Epoch 189/400, Loss: 2.302706969732588\n",
            "Epoch 190/400, Loss: 2.3027191277254713\n",
            "Epoch 191/400, Loss: 2.3027117442000997\n",
            "Epoch 192/400, Loss: 2.3027204397049816\n",
            "Epoch 193/400, Loss: 2.3027089204300535\n",
            "Epoch 194/400, Loss: 2.3027142306620423\n",
            "Epoch 195/400, Loss: 2.302717313170433\n",
            "Epoch 196/400, Loss: 2.302709674293345\n",
            "Epoch 197/400, Loss: 2.3027115308425645\n",
            "Epoch 198/400, Loss: 2.302708839828318\n",
            "Epoch 199/400, Loss: 2.302707249468023\n",
            "Epoch 200/400, Loss: 2.3027114407582716\n",
            "Epoch 201/400, Loss: 2.302712081508203\n",
            "Epoch 202/400, Loss: 2.302697370675477\n",
            "Epoch 203/400, Loss: 2.302710936828093\n",
            "Epoch 204/400, Loss: 2.3027088113806466\n",
            "Epoch 205/400, Loss: 2.3027153252200647\n",
            "Epoch 206/400, Loss: 2.3027109029618176\n",
            "Epoch 207/400, Loss: 2.302709723738107\n",
            "Epoch 208/400, Loss: 2.302708941427144\n",
            "Epoch 209/400, Loss: 2.3027127784761516\n",
            "Epoch 210/400, Loss: 2.3027092292904854\n",
            "Epoch 211/400, Loss: 2.302719155495817\n",
            "Epoch 212/400, Loss: 2.302709937095642\n",
            "Epoch 213/400, Loss: 2.302711333063516\n",
            "Epoch 214/400, Loss: 2.302710436284542\n",
            "Epoch 215/400, Loss: 2.302713646807454\n",
            "Epoch 216/400, Loss: 2.302704160863703\n",
            "Epoch 217/400, Loss: 2.3027111901478334\n",
            "Epoch 218/400, Loss: 2.302716973830353\n",
            "Epoch 219/400, Loss: 2.3027077195319263\n",
            "Epoch 220/400, Loss: 2.3027105040170928\n",
            "Epoch 221/400, Loss: 2.302706150846048\n",
            "Epoch 222/400, Loss: 2.3027137301184912\n",
            "Epoch 223/400, Loss: 2.3027073111046445\n",
            "Epoch 224/400, Loss: 2.3027119799093767\n",
            "Epoch 225/400, Loss: 2.302712682295929\n",
            "Epoch 226/400, Loss: 2.302712343633175\n",
            "Epoch 227/400, Loss: 2.302716079083356\n",
            "Epoch 228/400, Loss: 2.302716827528043\n",
            "Epoch 229/400, Loss: 2.30271416767077\n",
            "Epoch 230/400, Loss: 2.302707714113322\n",
            "Epoch 231/400, Loss: 2.302713995630091\n",
            "Epoch 232/400, Loss: 2.3027094412933695\n",
            "Epoch 233/400, Loss: 2.302710313688625\n",
            "Epoch 234/400, Loss: 2.302710222249681\n",
            "Epoch 235/400, Loss: 2.302707000212236\n",
            "Epoch 236/400, Loss: 2.3027110411362215\n",
            "Epoch 237/400, Loss: 2.302708134732463\n",
            "Epoch 238/400, Loss: 2.302712755447084\n",
            "Epoch 239/400, Loss: 2.3027122102000495\n",
            "Epoch 240/400, Loss: 2.302710187028755\n",
            "Epoch 241/400, Loss: 2.3027062957937066\n",
            "Epoch 242/400, Loss: 2.3027076734737917\n",
            "Epoch 243/400, Loss: 2.302703022279523\n",
            "Epoch 244/400, Loss: 2.3027153861793606\n",
            "Epoch 245/400, Loss: 2.302708937363191\n",
            "Epoch 246/400, Loss: 2.3027116751128975\n",
            "Epoch 247/400, Loss: 2.3027136542580346\n",
            "Epoch 248/400, Loss: 2.302710684185678\n",
            "Epoch 249/400, Loss: 2.3027143986387686\n",
            "Epoch 250/400, Loss: 2.302713980051604\n",
            "Epoch 251/400, Loss: 2.302708873017268\n",
            "Epoch 252/400, Loss: 2.3027007620443\n",
            "Epoch 253/400, Loss: 2.302712594920939\n",
            "Epoch 254/400, Loss: 2.30271316929297\n",
            "Epoch 255/400, Loss: 2.302708129313859\n",
            "Epoch 256/400, Loss: 2.302705669267611\n",
            "Epoch 257/400, Loss: 2.302708299322562\n",
            "Epoch 258/400, Loss: 2.302710317752578\n",
            "Epoch 259/400, Loss: 2.302707439796491\n",
            "Epoch 260/400, Loss: 2.302716900679198\n",
            "Epoch 261/400, Loss: 2.3027091669765385\n",
            "Epoch 262/400, Loss: 2.302719068798152\n",
            "Epoch 263/400, Loss: 2.3027150949293915\n",
            "Epoch 264/400, Loss: 2.302714124999263\n",
            "Epoch 265/400, Loss: 2.302710233764215\n",
            "Epoch 266/400, Loss: 2.3027190274812956\n",
            "Epoch 267/400, Loss: 2.3027125495401295\n",
            "Epoch 268/400, Loss: 2.302710610357198\n",
            "Epoch 269/400, Loss: 2.3027184029871766\n",
            "Epoch 270/400, Loss: 2.3027218370275064\n",
            "Epoch 271/400, Loss: 2.302712149240754\n",
            "Epoch 272/400, Loss: 2.3027135424993257\n",
            "Epoch 273/400, Loss: 2.302714203569022\n",
            "Epoch 274/400, Loss: 2.3027139245109125\n",
            "Epoch 275/400, Loss: 2.302702457390048\n",
            "Epoch 276/400, Loss: 2.302716916257685\n",
            "Epoch 277/400, Loss: 2.3027120794762266\n",
            "Epoch 278/400, Loss: 2.3027098937468096\n",
            "Epoch 279/400, Loss: 2.302712871947072\n",
            "Epoch 280/400, Loss: 2.3027097189968284\n",
            "Epoch 281/400, Loss: 2.3027112267234107\n",
            "Epoch 282/400, Loss: 2.302714253013784\n",
            "Epoch 283/400, Loss: 2.302713115784255\n",
            "Epoch 284/400, Loss: 2.302703969180584\n",
            "Epoch 285/400, Loss: 2.30271321602843\n",
            "Epoch 286/400, Loss: 2.3027095794677734\n",
            "Epoch 287/400, Loss: 2.302705504000187\n",
            "Epoch 288/400, Loss: 2.3027119873599573\n",
            "Epoch 289/400, Loss: 2.302705899558284\n",
            "Epoch 290/400, Loss: 2.3027114407582716\n",
            "Epoch 291/400, Loss: 2.302712405269796\n",
            "Epoch 292/400, Loss: 2.3027111996303904\n",
            "Epoch 293/400, Loss: 2.302718984809789\n",
            "Epoch 294/400, Loss: 2.3027097819881006\n",
            "Epoch 295/400, Loss: 2.302718209272081\n",
            "Epoch 296/400, Loss: 2.3027050230990755\n",
            "Epoch 297/400, Loss: 2.302712444554676\n",
            "Epoch 298/400, Loss: 2.3027098070491445\n",
            "Epoch 299/400, Loss: 2.30271024053747\n",
            "Epoch 300/400, Loss: 2.3027157228101385\n",
            "Epoch 301/400, Loss: 2.302716586400162\n",
            "Epoch 302/400, Loss: 2.302704784003171\n",
            "Epoch 303/400, Loss: 2.302710058336908\n",
            "Epoch 304/400, Loss: 2.302718299356374\n",
            "Epoch 305/400, Loss: 2.3027109422466974\n",
            "Epoch 306/400, Loss: 2.3027101226828317\n",
            "Epoch 307/400, Loss: 2.30270384455269\n",
            "Epoch 308/400, Loss: 2.30270782451738\n",
            "Epoch 309/400, Loss: 2.3027077459476213\n",
            "Epoch 310/400, Loss: 2.3027158067985014\n",
            "Epoch 311/400, Loss: 2.302713405002247\n",
            "Epoch 312/400, Loss: 2.3027099045840176\n",
            "Epoch 313/400, Loss: 2.3027133237231863\n",
            "Epoch 314/400, Loss: 2.3027109408920463\n",
            "Epoch 315/400, Loss: 2.3027056083083153\n",
            "Epoch 316/400, Loss: 2.3027096993543883\n",
            "Epoch 317/400, Loss: 2.3027089021422644\n",
            "Epoch 318/400, Loss: 2.302714156833562\n",
            "Epoch 319/400, Loss: 2.302719410170208\n",
            "Epoch 320/400, Loss: 2.302710648964752\n",
            "Epoch 321/400, Loss: 2.3027164272286673\n",
            "Epoch 322/400, Loss: 2.3027105141769755\n",
            "Epoch 323/400, Loss: 2.3027104715054687\n",
            "Epoch 324/400, Loss: 2.3027198016643524\n",
            "Epoch 325/400, Loss: 2.3027105175636033\n",
            "Epoch 326/400, Loss: 2.3027147020805967\n",
            "Epoch 327/400, Loss: 2.30271109870889\n",
            "Epoch 328/400, Loss: 2.3027034144509924\n",
            "Epoch 329/400, Loss: 2.302704164927656\n",
            "Epoch 330/400, Loss: 2.3027142773975027\n",
            "Epoch 331/400, Loss: 2.3027056990699335\n",
            "Epoch 332/400, Loss: 2.302709139206193\n",
            "Epoch 333/400, Loss: 2.3027147582986136\n",
            "Epoch 334/400, Loss: 2.302716386589137\n",
            "Epoch 335/400, Loss: 2.3027074242180046\n",
            "Epoch 336/400, Loss: 2.302707340229641\n",
            "Epoch 337/400, Loss: 2.302711618217555\n",
            "Epoch 338/400, Loss: 2.3027085573835806\n",
            "Epoch 339/400, Loss: 2.3027140626853164\n",
            "Epoch 340/400, Loss: 2.3027157322926954\n",
            "Epoch 341/400, Loss: 2.3027105649763886\n",
            "Epoch 342/400, Loss: 2.3027072487906977\n",
            "Epoch 343/400, Loss: 2.302713943476027\n",
            "Epoch 344/400, Loss: 2.302714053202759\n",
            "Epoch 345/400, Loss: 2.3027123253453863\n",
            "Epoch 346/400, Loss: 2.3027148477055808\n",
            "Epoch 347/400, Loss: 2.3027144501155075\n",
            "Epoch 348/400, Loss: 2.3027018891139464\n",
            "Epoch 349/400, Loss: 2.302718023007566\n",
            "Epoch 350/400, Loss: 2.3027090978893368\n",
            "Epoch 351/400, Loss: 2.3027113737030462\n",
            "Epoch 352/400, Loss: 2.302708835764365\n",
            "Epoch 353/400, Loss: 2.302713259377263\n",
            "Epoch 354/400, Loss: 2.3027155575427143\n",
            "Epoch 355/400, Loss: 2.302712040191347\n",
            "Epoch 356/400, Loss: 2.3027138649062677\n",
            "Epoch 357/400, Loss: 2.3027095943689346\n",
            "Epoch 358/400, Loss: 2.3027138554237108\n",
            "Epoch 359/400, Loss: 2.302706045860594\n",
            "Epoch 360/400, Loss: 2.302713407711549\n",
            "Epoch 361/400, Loss: 2.3027201003649016\n",
            "Epoch 362/400, Loss: 2.3027174039320513\n",
            "Epoch 363/400, Loss: 2.302699387750842\n",
            "Epoch 364/400, Loss: 2.302712183107029\n",
            "Epoch 365/400, Loss: 2.3027087822556496\n",
            "Epoch 366/400, Loss: 2.3027107525955546\n",
            "Epoch 367/400, Loss: 2.302705942229791\n",
            "Epoch 368/400, Loss: 2.302711359479211\n",
            "Epoch 369/400, Loss: 2.3027137714353474\n",
            "Epoch 370/400, Loss: 2.3027104728601198\n",
            "Epoch 371/400, Loss: 2.3027097203514795\n",
            "Epoch 372/400, Loss: 2.302704621445049\n",
            "Epoch 373/400, Loss: 2.302716193551367\n",
            "Epoch 374/400, Loss: 2.30270842327313\n",
            "Epoch 375/400, Loss: 2.302714738656174\n",
            "Epoch 376/400, Loss: 2.3027172948826444\n",
            "Epoch 377/400, Loss: 2.3027169040658255\n",
            "Epoch 378/400, Loss: 2.30270472981713\n",
            "Epoch 379/400, Loss: 2.302720743824135\n",
            "Epoch 380/400, Loss: 2.3027102425694466\n",
            "Epoch 381/400, Loss: 2.302708167921413\n",
            "Epoch 382/400, Loss: 2.302702995186502\n",
            "Epoch 383/400, Loss: 2.3027105534618553\n",
            "Epoch 384/400, Loss: 2.302717325362292\n",
            "Epoch 385/400, Loss: 2.302716629071669\n",
            "Epoch 386/400, Loss: 2.3027136664498937\n",
            "Epoch 387/400, Loss: 2.3027152940630913\n",
            "Epoch 388/400, Loss: 2.3027117306535896\n",
            "Epoch 389/400, Loss: 2.302705789154226\n",
            "Epoch 390/400, Loss: 2.3027156096967785\n",
            "Epoch 391/400, Loss: 2.302706454287876\n",
            "Epoch 392/400, Loss: 2.3027093850753526\n",
            "Epoch 393/400, Loss: 2.3027121384035456\n",
            "Epoch 394/400, Loss: 2.3027115884152325\n",
            "Epoch 395/400, Loss: 2.302711236205968\n",
            "Epoch 396/400, Loss: 2.3027148151939567\n",
            "Epoch 397/400, Loss: 2.302707799456336\n",
            "Epoch 398/400, Loss: 2.302703707055612\n",
            "Epoch 399/400, Loss: 2.302708440883593\n",
            "Epoch 400/400, Loss: 2.302718435498801\n",
            "Validation Accuracy: 9.58%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0e1e63393e85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Accuracy: {accuracy:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download and extract the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as archive:\n",
        "    archive.extractall(path='./data')\n",
        "\n",
        "# Load dataset\n",
        "train_data = ImageFolder('data/cifar10/train', transform=ToTensor())\n",
        "print(train_data[1])\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "train_set, valid_set = random_split(train_data, [45000, 5000])\n",
        "print(len(train_set))\n",
        "print('\\n')\n",
        "print(len(valid_set))\n",
        "\n",
        "# DataLoader setup\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# CNN Model definition\n",
        "class CIFAR10_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # Adjust the dimensions after conv layers\n",
        "        self.fc2 = nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10_CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to GPU\n",
        "model = CIFAR10_CNN().to(device)\n",
        "\n",
        "# Update train and validation functions to move data to GPU\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=400):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=400)\n",
        "def validate_model(model, valid_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "validate_model(model, valid_loader)\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "test_model(model, test_loader)"
      ]
    }
  ]
}